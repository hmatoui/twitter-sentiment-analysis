{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d29b4850",
   "metadata": {},
   "source": [
    "Load Necessary Libraries and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19890e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e59063",
   "metadata": {},
   "source": [
    "Load the cleaned data saved during the exploration step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce016c24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target          id                          date      flag  \\\n",
       "0       0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                               text  \\\n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...   \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...   \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...   \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire    \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....   \n",
       "\n",
       "  sentiment  \n",
       "0  Negative  \n",
       "1  Negative  \n",
       "2  Negative  \n",
       "3  Negative  \n",
       "4  Negative  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset\n",
    "data_path = \"../data/processed/cleaned_sentiment140.csv\"\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Check loaded data\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b44619",
   "metadata": {},
   "source": [
    "Define Cleaning Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0985e908",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, stem=False):\n",
    "    \"\"\"\n",
    "    Cleans the input text by removing URLs, special characters, and stopwords.\n",
    "    \"\"\"\n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Remove user mentions and hashtags\n",
    "    text = re.sub(r\"@\\w+|#\\w+\", '', text)\n",
    "    \n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", '', text)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Tokenize and remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    filtered_words = []\n",
    "    for word in text.split():\n",
    "        if word not in stop_words:\n",
    "            if stem:\n",
    "                filtered_words.append(stemmer.stem(word))\n",
    "            else:\n",
    "                filtered_words.append(word)\n",
    "    \n",
    "    return ' '.join(filtered_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fda050a",
   "metadata": {},
   "source": [
    "Apply Cleaning\n",
    "\n",
    "Example Before and After\n",
    "\n",
    "Original: \"RT @user: Check out our new product! http://example.com\"\n",
    "\n",
    "Cleaned: \"check new product\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3406dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1581466/1581466 [06:25<00:00, 4099.63it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "# Clean tweets\n",
    "data['cleaned_text'] = data['text'].progress_apply(clean_text)\n",
    "\n",
    "# Save the intermediate processed dataset\n",
    "data.to_csv(\"../data/processed/cleaned_texts.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202a5195",
   "metadata": {},
   "source": [
    "Tokenize cleaned text to prepare for embedding generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2013f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize each cleaned tweet\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(data['cleaned_text'])\n",
    "data['tokens'] = tokenizer.texts_to_sequences(data['cleaned_text'])\n",
    "\n",
    "# To ensure that all sequences are of the same length (necessary for many deep learning models), pad them\n",
    "max_length = 300  # Set this based on the average tweet length in your dataset\n",
    "data['padded_tokens'] = pad_sequences(data['tokens'], maxlen=max_length, padding='post').tolist()  # Pad sequences\n",
    "\n",
    "# Save tokenized data\n",
    "data.to_csv(\"../data/processed/tokenized_texts.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1550820f",
   "metadata": {},
   "source": [
    "**Word2Vec Embeddings**\n",
    "\n",
    "Train Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6728ac03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Prepare list of tokenized sentences\n",
    "tokenized_texts = data['padded_tokens'].tolist()\n",
    "\n",
    "# Train Word2Vec model\n",
    "word2vec_model = Word2Vec(sentences=tokenized_texts, vector_size=300, window=5, min_count=5, workers=4)\n",
    "\n",
    "# Save the model\n",
    "word2vec_model.save(\"../models/word2vec/word2vec.model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab75ab49",
   "metadata": {},
   "source": [
    "Generate Embedding for Tweets: Average the embeddings of words in a tweet to get a tweet-level embedding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99a14ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec_model = Word2Vec.load('../models/word2vec/word2vec.model')\n",
    "# data = pd.read_csv('../data/processed/tokenized_texts.csv')\n",
    "def tweet_embedding(tokens, model):\n",
    "    \"\"\"\n",
    "    Generates tweet-level embedding by averaging word embeddings.\n",
    "    \"\"\"\n",
    "    valid_embeddings = [model.wv[word] for word in tokens if word in model.wv]\n",
    "    if valid_embeddings:\n",
    "        return np.mean(valid_embeddings, axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)\n",
    "\n",
    "# Compute embeddings for all tweets\n",
    "data['embedding'] = data['padded_tokens'].apply(lambda x: tweet_embedding(x, word2vec_model))\n",
    "\n",
    "# Save tweet embeddings\n",
    "data['embedding'] = data['embedding'].apply(lambda x: json.dumps(x.tolist()) if isinstance(x, np.ndarray) else x)\n",
    "data[['embedding', 'target']].to_csv(\"../data/processed/tweet_embeddings.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
